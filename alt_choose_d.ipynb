{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import custom class\n",
    "from defs import KernelApprox\n",
    "\n",
    "#importing libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy \n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "from itertools import chain, combinations #import function for powerset\n",
    "\n",
    "import concurrent.futures #import function for parallelization\n",
    "import multiprocessing #for getting number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining functions\n",
    "def f(y): \n",
    "    fy = y**4 - 2*y**3 + y**2 -1/30\n",
    "    try:\n",
    "        d = y.shape[1]\n",
    "        return np.prod(1 + (fy/(np.arange(1,d+1)**4)), axis=1, keepdims=True)\n",
    "    except: #1d\n",
    "        return 1 + fy\n",
    "\n",
    "def g(y, lam = 0.01):\n",
    "    return abs(np.cos(lam*2*np.pi*y + 0.5*np.pi) )/( lam*2*np.pi )\n",
    "\n",
    "\n",
    "#function for generating samples\n",
    "def gen_samples(f, K, d, const = 1.1):\n",
    "    extra = const**2 + 0.05 + 1/np.sqrt(K*0.1) #tends to const**2 + 0.05 as K -> inf\n",
    "    K_adj = int(K*extra)\n",
    "\n",
    "    u = scipy.stats.uniform.rvs(size = K_adj)\n",
    "    y = scipy.stats.uniform.rvs(size = (K_adj, d))\n",
    "\n",
    "    fy = f(y)\n",
    "\n",
    "    Y_est = y[np.where(np.less(u,fy[:,0]/(const**2)))]\n",
    "    return Y_est[:K]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example use of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1 #dimension\n",
    "alpha = 4 #smoothing parameter\n",
    "\n",
    "M = int(1e7) #number of samples\n",
    "N = 11 #number of points to evaluate at (should be prime). If N not given, will be chosen automatically based on M\n",
    "lam = 0.01 #regularisation parameter. If lam not given, will be chosen automatically based on M\n",
    "\n",
    "#number of cores to use for parallelization\n",
    "num_workers = 12 #if not given, will be automatically chosen (will not use more than N cores)\n",
    "\n",
    "#generate samples\n",
    "Y = gen_samples(f, M, d)\n",
    "\n",
    "#create class instance\n",
    "approx = KernelApprox(Y, lamda = lam, n = N, alpha = 4, num_workers = 12, verbose = True)\n",
    "\n",
    "approx.gen_c() #generate c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([np.linspace(0,1,1000)]).T #points to evaluate at\n",
    "approx.kern_est(y) #estimate kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate on a grid with 100 points\n",
    "approx.kern_est_grid(100)\n",
    "\n",
    "approx.plot_est() #plots estimate of kern_est_grid in 1d or 2d (will run kern_est_grid if not already run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate MISE\n",
    "approx.calc_error_L2(f) #calculate L2 error\n",
    "\n",
    "#approx.L2_error #error estimate\n",
    "#approx.L2_error_var #variance of error estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate weak error\n",
    "approx.calc_error(f, g, K = int(1e7)) #calculate weak error\n",
    "\n",
    "#approx.error #error estimate\n",
    "#approx.error_var #variance of error estimate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for looking at error convergence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N & M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 6\n",
    "alpha = 4\n",
    "\n",
    "#number of workers for parallel processing\n",
    "num_workers = 12\n",
    "\n",
    "#number of times to compute the error for each size\n",
    "num_intervals = 100\n",
    "\n",
    "#constant for sampling\n",
    "samp_const = 1.2  #(should be > f_max)\n",
    "\n",
    "#values to iterate over\n",
    "size = np.arange(1,7+1) # M = 10**size is the number of samples\n",
    "K_size = 7 # K = 10**K_size is the number of samples used to compute the kernel estimate\n",
    "num_points =  np.array([2,3,5,7,11,13,17,19,23]) #number of lattice points\n",
    "\n",
    "alpha_text = \"\"\n",
    "if alpha != 4:\n",
    "    alpha_text = \"\".join([str(alpha), \"_\", alpha_text])\n",
    "\n",
    "#vector for storing the errors (means)\n",
    "mean_err = np.zeros((len(num_points), len(size))).T\n",
    "mean_err_var = np.zeros((len(num_points), len(size))).T\n",
    "mean_err_M_var = np.zeros((len(num_points), len(size))).T\n",
    "mean_L2_err = np.zeros((len(num_points), len(size))).T\n",
    "mean_L2_err_var = np.zeros((len(num_points), len(size))).T\n",
    "mean_L2_err_M_var = np.zeros((len(num_points), len(size))).T\n",
    "mean_sum_c = np.zeros((len(num_points), len(size))).T\n",
    "\n",
    "\n",
    "#lattice for L2 error\n",
    "Y = gen_samples(f, int(1e2), d)\n",
    "approx = KernelApprox(Y, 0.01, n = int(1e4), alpha = alpha, num_workers = num_workers)\n",
    "approx.gen_lattice()\n",
    "lattice = approx.lattice\n",
    "\n",
    "for j in tqdm(range(len(size))): #iterate over the number of samples (M)\n",
    "    print('='*80) #for clarity (splits iterations)\n",
    "    print(\"M = 10^\", size[j])\n",
    "    M = int(10**size[j]) #number of samples\n",
    "\n",
    "    #adjust based on M to get good estimates of the error\n",
    "    num_intervals_adjusted = num_intervals #* max( 5 - j , 1)\n",
    "\n",
    "    #vector for storing the errors\n",
    "    err = np.zeros((len(num_points), num_intervals_adjusted))\n",
    "    err_var = np.zeros((len(num_points), num_intervals_adjusted))\n",
    "    L2_err = np.zeros((len(num_points), num_intervals_adjusted))\n",
    "    L2_err_var = np.zeros((len(num_points), num_intervals_adjusted))\n",
    "    sum_c = np.zeros((len(num_points), num_intervals_adjusted))\n",
    "\n",
    "    for num in tqdm(range(num_intervals_adjusted)): #iterate over the number of times to compute the error for each size\n",
    "\n",
    "\n",
    "        #generate the samples\n",
    "        Y = gen_samples(f, M, d)\n",
    "\n",
    "        for i in tqdm(range(len(num_points)), leave = False, disable=True): #iterate over the number of lattice points (n)\n",
    "        \n",
    "            #####################################\n",
    "                \n",
    "            #compute the kernel estimate\n",
    "            approx = KernelApprox(Y, 0.01, n = num_points[i], alpha = alpha, num_workers = num_workers, verbose=False)\n",
    "            approx.gen_c()\n",
    "\n",
    "            np.save(\"\".join([\"c_\", str(d), \"d.npy\"]), approx.c) #SAVE THE C VALUES\n",
    "\n",
    "            ##################################\n",
    "            K = int(10**K_size) #number of samples used to compute the kernel estimate (K)\n",
    "            \n",
    "            err[i, num] = approx.calc_error(f, g, K, num_workers = num_workers, const = samp_const)\n",
    "            err_var[i, num] = approx.error_var\n",
    "\n",
    "            L2_err[i, num] = approx.calc_error_L2(f, N = int(1e4), x = lattice)\n",
    "            L2_err_var[i, num] = approx.L2_error_var\n",
    "\n",
    "            sum_c[i, num] = sum(approx.c)\n",
    "            \n",
    "            mean_err[j] = np.mean(  ( err ), axis = 1)\n",
    "            mean_err_var[j] = np.mean(  ( err_var ), axis = 1)\n",
    "            mean_err_M_var[j] = np.var(  ( err ), axis = 1)\n",
    "            mean_L2_err[j] = np.mean(  ( L2_err ), axis = 1)\n",
    "            mean_L2_err_var[j] = np.mean(  ( L2_err_var ), axis = 1)\n",
    "            mean_L2_err_M_var[j] = np.var(  ( L2_err ), axis = 1)\n",
    "            mean_sum_c[j] = np.mean(  ( sum_c ), axis = 1)\n",
    "\n",
    "            np.save(\"\".join([\"NM_\", alpha_text, \"error_\", str(d), \"d.npy\"]), mean_err) #SAVE THE ERRORS\n",
    "            np.save(\"\".join([\"NM_\", alpha_text, \"error_var\", str(d), \"d.npy\"]), mean_err_var) #SAVE THE ERRORS\n",
    "            np.save(\"\".join([\"NM_\", alpha_text, \"error_M_var\", str(d), \"d.npy\"]), mean_err_M_var) #SAVE THE ERRORS\n",
    "            \n",
    "\n",
    "            np.save(\"\".join([\"NM_\", alpha_text, \"L2_error_\", str(d), \"d.npy\"]), mean_L2_err) #SAVE THE ERRORS\n",
    "            np.save(\"\".join([\"NM_\", alpha_text, \"L2_error_var\", str(d), \"d.npy\"]), mean_L2_err_var) #SAVE THE ERRORS\n",
    "            np.save(\"\".join([\"NM_\", alpha_text, \"L2_error_M_var\", str(d), \"d.npy\"]), mean_L2_err_M_var) #SAVE THE ERRORS\n",
    "\n",
    "            np.save(\"\".join([\"NM_\", alpha_text, \"sum_c\", str(d), \"d.npy\"]), mean_sum_c)\n",
    "            ##################################\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce plots from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 6\n",
    "alpha = 4\n",
    "\n",
    "#number of workers for parallel processing\n",
    "num_workers = 7\n",
    "\n",
    "#number of times to compute the error for each size\n",
    "num_intervals = 20\n",
    "\n",
    "#constant for sampling\n",
    "samp_const = 1.2  #(should be > f_max) \n",
    "# 1.1 for standard f\n",
    "\n",
    "#values to iterate over\n",
    "size = np.arange(1,7+1) # M = 10**size is the number of samples\n",
    "K_size = 7 # K = 10**K_size is the number of samples used to compute the kernel estimate\n",
    "num_points =  np.array([5,7,11]) #number of lattice points\n",
    "\n",
    "lams = np.array([0.8, 0.4, 0.1, 0.01]) #lambda for the kernel\n",
    "lam_texts = [\"8\", \"4\", \"1\", \"01\"] #lambda for the kernel (for saving)\n",
    "\n",
    "\n",
    "#vector for storing the errors (means)\n",
    "mean_err = np.zeros((len(num_points), len(size))).T\n",
    "mean_err_var = np.zeros((len(num_points), len(size))).T\n",
    "mean_err_M_var = np.zeros((len(num_points), len(size))).T\n",
    "mean_L2_err = np.zeros((len(num_points), len(size))).T\n",
    "mean_L2_err_var = np.zeros((len(num_points), len(size))).T\n",
    "mean_L2_err_M_var = np.zeros((len(num_points), len(size))).T\n",
    "\n",
    "\n",
    "#lattice for L2 error\n",
    "Y = gen_samples(f, int(1e2), d)\n",
    "approx = KernelApprox(Y, 0.01, n = int(1e4), alpha = alpha, num_workers = num_workers)\n",
    "approx.gen_lattice()\n",
    "lattice = approx.lattice\n",
    "\n",
    "for l in tqdm(range(len(lams))):\n",
    "    lam = lams[l]\n",
    "    lam_text = lam_texts[l]\n",
    "    if alpha != 4:\n",
    "        lam_text = \"\".join([lam_text, \"_\", str(alpha)])\n",
    "\n",
    "    for j in tqdm(range(len(size))): #iterate over the number of samples (M)\n",
    "        print('='*80) #for clarity (splits iterations)\n",
    "        print(\"M = 10^\", size[j])\n",
    "        M = int(10**size[j]) #number of samples\n",
    "\n",
    "        #adjust based on M to get good estimates of the error\n",
    "        num_intervals_adjusted = num_intervals #* max( 5 - j , 1)\n",
    "\n",
    "        #vector for storing the errors\n",
    "        err = np.zeros((len(num_points), num_intervals_adjusted))\n",
    "        err_var = np.zeros((len(num_points), num_intervals_adjusted))\n",
    "        L2_err = np.zeros((len(num_points), num_intervals_adjusted))\n",
    "        L2_err_var = np.zeros((len(num_points), num_intervals_adjusted))\n",
    "\n",
    "        for num in tqdm(range(num_intervals_adjusted)): #iterate over the number of times to compute the error for each size\n",
    "\n",
    "            #generate the samples\n",
    "            Y = gen_samples(f, M, d)\n",
    "\n",
    "            for i in tqdm(range(len(num_points)), leave = False, disable=True): #iterate over the number of lattice points (n)\n",
    "            \n",
    "                #####################################\n",
    "                    \n",
    "                #compute the kernel estimate\n",
    "                approx = KernelApprox(Y, lam, n = num_points[i], alpha = alpha, num_workers = num_workers, verbose=False)\n",
    "                approx.gen_c()\n",
    "\n",
    "                np.save(\"\".join([\"c_\", str(d), \"d.npy\"]), approx.c) #SAVE THE C VALUES\n",
    "\n",
    "                ##################################\n",
    "                K = int(10**K_size) #number of samples used to compute the kernel estimate (K)\n",
    "                \n",
    "                err[i, num] = approx.calc_error(f, g, K, num_workers = num_workers, const = samp_const)\n",
    "                err_var[i, num] = approx.error_var\n",
    "\n",
    "                L2_err[i, num] = approx.calc_error_L2(f, N = int(1e4), x = lattice)\n",
    "                L2_err_var[i, num] = approx.L2_error_var\n",
    "                \n",
    "                mean_err[j] = np.mean(  ( err ), axis = 1)\n",
    "                mean_err_var[j] = np.mean(  ( err_var ), axis = 1)\n",
    "                mean_err_M_var[j] = np.var(  ( err ), axis = 1)\n",
    "                mean_L2_err[j] = np.mean(  ( L2_err ), axis = 1)\n",
    "                mean_L2_err_var[j] = np.mean(  ( L2_err_var ), axis = 1)\n",
    "                mean_L2_err_M_var[j] = np.var(  ( L2_err ), axis = 1)\n",
    "\n",
    "            np.save(\"\".join([\"NM_\", lam_text,\"_error_\", str(d), \"d.npy\"]), mean_err) #SAVE THE ERRORS\n",
    "            np.save(\"\".join([\"NM_\", lam_text,\"_error_var\", str(d), \"d.npy\"]), mean_err_var) #SAVE THE ERRORS\n",
    "            np.save(\"\".join([\"NM_\", lam_text,\"_error_M_var\", str(d), \"d.npy\"]), mean_err_M_var) #SAVE THE ERRORS\n",
    "                \n",
    "\n",
    "            np.save(\"\".join([\"NM_\", lam_text,\"_L2_error_\", str(d), \"d.npy\"]), mean_L2_err) #SAVE THE ERRORS\n",
    "            np.save(\"\".join([\"NM_\", lam_text,\"_L2_error_var\", str(d), \"d.npy\"]), mean_L2_err_var) #SAVE THE ERRORS\n",
    "            np.save(\"\".join([\"NM_\", lam_text,\"_L2_error_M_var\", str(d), \"d.npy\"]), mean_L2_err_M_var) #SAVE THE ERRORS\n",
    "                ##################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 6\n",
    "alpha = 4\n",
    "\n",
    "#number of workers for parallel processing\n",
    "num_workers = 12\n",
    "\n",
    "#number of times to compute the error for each size\n",
    "num_intervals = 20\n",
    "\n",
    "#constant for sampling\n",
    "samp_const = 1.2  #(should be > f_max) \n",
    "# 1.1 for standard f\n",
    "\n",
    "#values to iterate over\n",
    "size =  7 # M = 10**size is the number of samples\n",
    "K_size = 7 # K = 10**K_size is the number of samples used to compute the kernel estimate\n",
    "num_points =  23 #number of lattice points\n",
    "\n",
    "lams = 0.7**np.arange(70+1) #lambda for the kernel\n",
    "\n",
    "\n",
    "\n",
    "#vector for storing the errors (means)\n",
    "mean_err = np.zeros(len(lams))\n",
    "mean_err_var = np.zeros(len(lams))\n",
    "mean_err_M_var = np.zeros(len(lams))\n",
    "mean_L2_err = np.zeros(len(lams))\n",
    "mean_L2_err_var = np.zeros(len(lams))\n",
    "mean_L2_err_M_var = np.zeros(len(lams))\n",
    "\n",
    "#lattice for L2 error\n",
    "Y = gen_samples(f, int(1e2), d)\n",
    "approx = KernelApprox(Y, 0.01, n = int(1e4), alpha = alpha, num_workers = num_workers)\n",
    "approx.gen_lattice()\n",
    "lattice = approx.lattice\n",
    "\n",
    "lam_text = \"\"\n",
    "if alpha != 4:\n",
    "    lam_text = \"\".join([lam_text, \"_\", str(alpha)])\n",
    "\n",
    "\n",
    "\n",
    "M = int(10**size) #number of samples\n",
    "\n",
    "#vector for storing the errors\n",
    "err = np.zeros((len(lams), num_intervals))\n",
    "err_var = np.zeros((len(lams), num_intervals))\n",
    "L2_err = np.zeros((len(lams), num_intervals))\n",
    "L2_err_var = np.zeros((len(lams), num_intervals))\n",
    "\n",
    "for num in tqdm(range(num_intervals)): #iterate over the number of times to compute the error for each size\n",
    "\n",
    "    #generate the samples\n",
    "    Y = gen_samples(f, M, d)\n",
    "\n",
    "    for i in tqdm(range(len(lams)), leave = False, disable=False): #iterate over the number of lattice points (n)\n",
    "                #####################################\n",
    "                    \n",
    "        #compute the kernel estimate\n",
    "        approx = KernelApprox(Y, lams[i], n = num_points, alpha = alpha, num_workers = num_workers, verbose=False)\n",
    "        approx.gen_c()\n",
    "\n",
    "                ##################################\n",
    "        K = int(10**K_size) #number of samples used to compute the kernel estimate (K)\n",
    "                \n",
    "        err[i, num] = approx.calc_error(f, g, K, num_workers = num_workers, const = samp_const)\n",
    "        err_var[i, num] = approx.error_var\n",
    "\n",
    "        L2_err[i, num] = approx.calc_error_L2(f, N = int(1e4), x = lattice)\n",
    "        L2_err_var[i, num] = approx.L2_error_var\n",
    "                \n",
    "        mean_err = np.mean(  ( err ), axis = 1)\n",
    "        mean_err_var = np.mean(  ( err_var ), axis = 1)\n",
    "        mean_err_M_var = np.var(  ( err ), axis = 1)\n",
    "        mean_L2_err = np.mean(  ( L2_err ), axis = 1)\n",
    "        mean_L2_err_var = np.mean(  ( L2_err_var ), axis = 1)\n",
    "        mean_L2_err_M_var = np.var(  ( L2_err ), axis = 1)\n",
    "\n",
    "    np.save(\"\".join([\"lam_\", lam_text,\"_error_\", str(d), \"d.npy\"]), mean_err) #SAVE THE ERRORS\n",
    "    np.save(\"\".join([\"lam_\", lam_text,\"_error_var\", str(d), \"d.npy\"]), mean_err_var) #SAVE THE ERRORS\n",
    "    np.save(\"\".join([\"lam_\", lam_text,\"_error_M_var\", str(d), \"d.npy\"]), mean_err_M_var) #SAVE THE ERRORS\n",
    "                \n",
    "\n",
    "    np.save(\"\".join([\"lam_\", lam_text,\"_L2_error_\", str(d), \"d.npy\"]), mean_L2_err) #SAVE THE ERRORS\n",
    "    np.save(\"\".join([\"lam_\", lam_text,\"_L2_error_var\", str(d), \"d.npy\"]), mean_L2_err_var) #SAVE THE ERRORS\n",
    "    np.save(\"\".join([\"lam_\", lam_text,\"_L2_error_M_var\", str(d), \"d.npy\"]), mean_L2_err_M_var) #SAVE THE ERRORS\n",
    "                ##################################\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto selection of $\\lambda$ and $n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 6\n",
    "alpha = 4\n",
    "\n",
    "#number of workers for parallel processing\n",
    "num_workers = 12\n",
    "\n",
    "#number of times to compute the error for each size\n",
    "num_intervals = 20\n",
    "\n",
    "#constant for sampling\n",
    "samp_const = 1.2  #(should be > f_max) \n",
    "# 1.1 for standard f\n",
    "\n",
    "#values to iterate over\n",
    "size =  np.arange(1, 7+1) # M = 10**size is the number of samples\n",
    "K_size = 6 # K = 10**K_size is the number of samples used to compute the kernel estimate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#vector for storing the errors (means)\n",
    "mean_err = np.zeros(len(size))\n",
    "mean_err_var = np.zeros(len(size))\n",
    "mean_err_M_var = np.zeros(len(size))\n",
    "mean_L2_err = np.zeros(len(size))\n",
    "mean_L2_err_var = np.zeros(len(size))\n",
    "mean_L2_err_M_var = np.zeros(len(size))\n",
    "\n",
    "#get true integral\n",
    "#Y = gen_samples(f, int(1e8), d)\n",
    "#int_gf = np.mean(g(Y))\n",
    "#print(\"True integral:\", int_gf, \"+/-\", np.std(g(Y))/np.sqrt(1e8)) #print the true integral\n",
    "#int_gf = 0.08794891440769671\n",
    "\n",
    "#lattice for L2 error\n",
    "Y = gen_samples(f, int(1e2), d)\n",
    "approx = KernelApprox(Y, 0.01, n = int(1e4), alpha = alpha, num_workers = num_workers)\n",
    "approx.gen_lattice()\n",
    "lattice = approx.lattice\n",
    "\n",
    "lam_text = \"\"\n",
    "if alpha != 4:\n",
    "    lam_text = \"\".join([lam_text, \"_\", str(alpha)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#vector for storing the errors\n",
    "err = np.zeros((len(size), num_intervals))\n",
    "err_var = np.zeros((len(size), num_intervals))\n",
    "L2_err = np.zeros((len(size), num_intervals))\n",
    "L2_err_var = np.zeros((len(size), num_intervals))\n",
    "\n",
    "for num in tqdm(range(num_intervals)): #iterate over the number of times to compute the error for each size\n",
    "\n",
    "\n",
    "    for i in tqdm(range(len(size)), leave = False, disable=False): #iterate over the number of lattice points (n)\n",
    "                #####################################\n",
    "        \n",
    "        M = int(10**size[i]) #number of samples\n",
    "        #generate the samples\n",
    "        Y = gen_samples(f, M, d)\n",
    "\n",
    "        #compute the kernel estimate\n",
    "        approx = KernelApprox(Y, alpha = alpha, num_workers = num_workers, verbose=False)\n",
    "        approx.gen_c()\n",
    "\n",
    "        #print(\"M:\", M, \"; N:\", approx.n, \"; lam:\", approx.lamda)\n",
    "\n",
    "                ##################################\n",
    "        K = int(10**K_size) #number of samples used to compute the kernel estimate (K)\n",
    "                \n",
    "        err[i, num] = approx.calc_error(f, g, K, num_workers = num_workers, const = samp_const)\n",
    "        err_var[i, num] = approx.error_var\n",
    "\n",
    "        L2_err[i, num] = approx.calc_error_L2(f, N = int(1e4), x = lattice)\n",
    "        L2_err_var[i, num] = approx.L2_error_var\n",
    "                \n",
    "        mean_err = np.mean(  ( err ), axis = 1)\n",
    "        mean_err_var = np.mean(  ( err_var ), axis = 1)\n",
    "        mean_err_M_var = np.var(  ( err ), axis = 1)\n",
    "        mean_L2_err = np.mean(  ( L2_err ), axis = 1)\n",
    "        mean_L2_err_var = np.mean(  ( L2_err_var ), axis = 1)\n",
    "        mean_L2_err_M_var = np.var(  ( L2_err ), axis = 1)\n",
    "\n",
    "    np.save(\"\".join([\"M\", lam_text,\"_error_\", str(d), \"d.npy\"]), mean_err) #SAVE THE ERRORS\n",
    "    np.save(\"\".join([\"M\", lam_text,\"_error_var\", str(d), \"d.npy\"]), mean_err_var) #SAVE THE ERRORS\n",
    "    np.save(\"\".join([\"M\", lam_text,\"_error_M_var\", str(d), \"d.npy\"]), mean_err_M_var) #SAVE THE ERRORS\n",
    "                \n",
    "\n",
    "    np.save(\"\".join([\"M\", lam_text,\"_L2_error_\", str(d), \"d.npy\"]), mean_L2_err) #SAVE THE ERRORS\n",
    "    np.save(\"\".join([\"M\", lam_text,\"_L2_error_var\", str(d), \"d.npy\"]), mean_L2_err_var) #SAVE THE ERRORS\n",
    "    np.save(\"\".join([\"M\", lam_text,\"_L2_error_M_var\", str(d), \"d.npy\"]), mean_L2_err_M_var) #SAVE THE ERRORS\n",
    "                ##################################\n",
    "\n",
    "\n",
    "                #print('-'*80) #dashes for clarity\n",
    "            #print('*'*80) #asterisks for clarity (splits iterations)\n",
    "        #print('='*80) #for clarity (splits iterations)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
